{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "梁老师你好：\n",
    "这里放置了学习过的3个基础机器学习算法的原理部分代码，掌握的比较熟练。这些代码是老师讲义，个人作业的拼接，但是框架是老师给出的。\n",
    "在学习过程中还有一些数据可视化方面的尝试和代码，以及简单的优化，就不加入这里了。\n",
    "可以看出这里的算法非常基础，但能够提供给我一些对机器学习的理解。\n",
    "对其他算法也有了解，但是都有些不完全理解的部分，没有展示，例如svm，不太理解感知基到向量机的转变与对偶性的数学部分，logistic regression的信息熵部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "面向对象，KNN具有三个属性: k, X_train, y_train \n",
    "有两个方法: fit 和 predict\n",
    "fit 有两个参数: X_train 和 y_train, 用来更新KNN的属性 X_train, y_train. 返回 KNN对象本身.\n",
    "predict 有一个参数: X_test, 在预测完 X_test 的标签(类别)后返回所有的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eu_dist(u, v):   #欧式距离，计算两个对象的空间距离，最简单的距离衡量\n",
    "    return np.sqrt(np.sum((u - v) ** 2))\n",
    "\n",
    "\n",
    "def most_frequent(ar):\n",
    "    u, indices = np.unique(ar, return_inverse=True)\n",
    "    return u[np.argmax(np.bincount(indices))]\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for i, unknown_sample in enumerate(X_test):\n",
    "            neighbours = []\n",
    "            for j, observed_sample in enumerate(self.X_train):\n",
    "                distance = eu_dist(unknown_sample, observed_sample)\n",
    "                neighbours.append([distance, self.y_train[j]])\n",
    "            neighbours = np.array(neighbours)\n",
    "            knn = neighbours[neighbours[:, 0].argsort()][:self.k]\n",
    "            predictions.append(most_frequent(knn[:, 1]))\n",
    "        return np.ravel(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "iris = pd.read_csv('bird.csv', delimiter=',')\n",
    "\n",
    "X = iris[['Sepal length', 'Sepal width',\n",
    "          'Petal length', 'Petal width']].as_matrix()\n",
    "y = iris['Species'].as_matrix()\n",
    "\n",
    "# 数据集分割, 67%的用于训练, 33%的用于测试\n",
    "# X_train, X_test = 训练样本, 测试样本\n",
    "# y_train, y_test = 训练目标, 测试目标\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=4200)     #可以设置不同的random state来进行交叉实验  \n",
    "\n",
    "knn = KNN(k=3)\n",
    "\n",
    "# 预测测试样本生成的目标: y_pred\n",
    "y_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "print('accuracy=%s' % accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression and gradient_descent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression: y = ([1,...,1]T|X)w\n",
    "损失函数L(w) =||Xw−y||22 , 最小化损失函数\n",
    "两种方法 close form和 gradient descent \n",
    "close form: ŵ = (X.T * X).inv * X.T * y   \n",
    "但在  X.T * X 是不可逆矩阵时 显式解不适用\n",
    "寻找最小梯度,即梯度最大方向的反方向 #公式打不出来\n",
    "alpha 为搜索步长，通过迭代更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:1000 \ttraining MSE=0.172 \tMAE=0.267 \tr^2=0.985\n",
      "iter:2000 \ttraining MSE=0.085 \tMAE=0.131 \tr^2=0.996\n",
      "iter:3000 \ttraining MSE=0.042 \tMAE=0.065 \tr^2=0.999\n",
      "iter:4000 \ttraining MSE=0.021 \tMAE=0.032 \tr^2=1.000\n",
      "iter:5000 \ttraining MSE=0.010 \tMAE=0.016 \tr^2=1.000\n",
      "iter:6000 \ttraining MSE=0.005 \tMAE=0.008 \tr^2=1.000\n",
      "iter:7000 \ttraining MSE=0.002 \tMAE=0.004 \tr^2=1.000\n",
      "iter:8000 \ttraining MSE=0.001 \tMAE=0.002 \tr^2=1.000\n",
      "iter:9000 \ttraining MSE=0.001 \tMAE=0.001 \tr^2=1.000\n",
      "iter:10000 \ttraining MSE=0.000 \tMAE=0.000 \tr^2=1.000\n",
      "fitted w is \t [ 2.42720014  0.28592049 -0.14260796]\n",
      "Am I correct? \t True\n",
      "training MSE=0.00 MAE=0.00 r^2=1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class LinearRegression_:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    @staticmethod\n",
    "    def ones_left(X):\n",
    "        X = np.array(X)\n",
    "        ones = np.ones(X.shape[0])\n",
    "        return np.column_stack([ones, X])\n",
    "    \n",
    "    @staticmethod\n",
    "    def closed_form(X ,y):\n",
    "        product = np.dot(X.T, X)\n",
    "        theInverse = np.linalg.inv(product)     #np.linalg.inv 即A的逆        显式解 : ŵ =(XTX)−1 * XTy\n",
    "        return np.dot(np.dot(theInverse, X.T), y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_descent(X, y, n_iters=10000, alpha=0.004, weight=None):\n",
    "        w = weight\n",
    "        if w is None:\n",
    "            w = np.random.rand(X.shape[1])\n",
    "            w = np.ones(X.shape[1])\n",
    "        pass\n",
    "    \n",
    "    \n",
    "        for i in range(1, n_iters+1):        \n",
    "            y_pred = X.dot(w)\n",
    "            loss = y_pred - y\n",
    "            \n",
    "            grad = loss.dot(X)/X.shape[0]\n",
    "            w = w - alpha *  grad         #使用迭代更新梯度  \n",
    "            \n",
    "            if i % (n_iters//10) == 0:\n",
    "                print('iter:%d \\ttraining MSE=%.3f \\tMAE=%.3f \\tr^2=%.3f'%(\n",
    "                    i,\n",
    "                    np.linalg.norm(loss),\n",
    "                    np.linalg.norm(loss, ord=1),\n",
    "                    r2_score(y, y_pred)\n",
    "                    )\n",
    "                )\n",
    "        return w\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, method='closed form', **kwargs):  # fit, 利用 X_train, y_train 计算  ŵ w^ , \n",
    "                                                                       #并将 ŵ ,w^ 更新到类的属性 self.w 中 \n",
    "\n",
    "        X = self.ones_left(X_train)\n",
    "        y = np.array(y_train)\n",
    "        \n",
    "        if method=='closed form':\n",
    "            self.w = self.closed_form(X ,y)\n",
    "        elif method == 'gradient descent':\n",
    "            self.w = self.gradient_descent(X, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        augX_test = self.ones_left(X_test)\n",
    "        return augX_test.dot(self.w)\n",
    "    \n",
    "# 测试\n",
    "import numpy as np\n",
    "\n",
    "mlr = LinearRegression_()\n",
    "\n",
    "X = np.array([[1, 5], [3, 2], [6, 1]])\n",
    "y = np.array([2, 3, 4])\n",
    "y_pred = mlr.fit(X, y, method='gradient descent', \n",
    "                 n_iters=10000, \n",
    "                 alpha=0.05).predict(X)\n",
    "print('fitted w is \\t', mlr.w)\n",
    "print('Am I correct? \\t', np.isclose(y, y_pred, atol=1e-2).all())\n",
    "print('training MSE=%.2f MAE=%.2f r^2=%.4f'% (\n",
    "        np.linalg.norm(y_pred - y),\n",
    "        np.linalg.norm(y_pred - y, ord=1),\n",
    "        r2_score(y_pred, y)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "依照基尼不纯度的指标，计算每种分类的基尼系数，找到最佳分枝\n",
    "如果使用树做回归, 即 y 为连续变量, 可以将impurity换成variance\n",
    "这里略了binary tree, 剪枝部分我不完全明白因此也略掉了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "credit = pd.read_excel('credit_risk.xlsx')\n",
    "X = credit[['less2yrs', 'missedpay']].as_matrix()\n",
    "y = credit['defaulted'].as_matrix()\n",
    "\n",
    "#此决策树使用gini index 作为indicate，\n",
    "def gini_impurity(y):\n",
    "#   assume y are all integers\n",
    "    p_i = np.bincount(y) / y.shape[0]\n",
    "    return 1 - np.sum(p_i ** 2)\n",
    "\n",
    "\n",
    "def gini_index(y_left, y_right):\n",
    "    l_count, r_count = y_left.shape[0], y_right.shape[0]\n",
    "    all_count = l_count + r_count\n",
    "    \n",
    "    l_imp = gini_impurity(y_left)\n",
    "    r_imp = gini_impurity(y_right)\n",
    "    \n",
    "    return l_count/all_count * l_imp + r_count/all_count * r_imp\n",
    "\n",
    "\n",
    "def get_best_split(X, y):\n",
    "    min_gini_index = 1.0\n",
    "    best_split = {}\n",
    "    for i in range(X.shape[1]): \n",
    "        thresholds = np.unique(X[:, i])\n",
    "        for th in thresholds:\n",
    "            left_mask = X[:, i] <= th\n",
    "            right_mask = X[:, i] > th\n",
    "\n",
    "            gi = gini_index(y[left_mask], y[right_mask])\n",
    "#             print('column=',i, 'thres=', th, 'gini_index=',gi)\n",
    "            if gi < min_gini_index:\n",
    "                min_gini_index = gi\n",
    "                best_split = {'left mask':left_mask, \n",
    "                              'right mask':right_mask, \n",
    "                              'feature index':i, \n",
    "                              'threshold':th, \n",
    "                              'gini index':gi}\n",
    "    return best_split\n",
    "\n",
    "\n",
    "def leaf_label(y):\n",
    "    return np.argmax(np.bincount(y))\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self, impurity=None):\n",
    "        self.feature_index = None   # 从第几个feature分割数据集\n",
    "        self.threshold = None       # 按照什么样的值分割数据集\n",
    "        self.label = None           # 如果节点是叶子, node具有对应的标签\n",
    "        self.true_branch = None     # 'Left' subtree\n",
    "        self.false_branch = None    # 'Right' subtree\n",
    "        self.impurity = impurity    # impurity最大是 1.0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ' '.join(['imp=%s'%(self.impurity), \n",
    "                         'feature=%s'%(self.feature_index), \n",
    "                         'threshold=%s'%(self.threshold),\n",
    "                         'label=%s'%(self.label),\n",
    "                         '\\n',\n",
    "                         'left %s'%(self.truebranch),\n",
    "                         'right %s'%(self.falsebranch)\n",
    "                        ])\n",
    "        \n",
    "def build_tree(X, y):\n",
    "    impurity = gini_impurity(y)\n",
    "    node = DecisionNode(impurity=impurity)\n",
    "    best_split = get_best_split(X, y)\n",
    "    \n",
    "    # 可以分叉\n",
    "    if best_split['gini index'] < impurity:\n",
    "        node.feature_index = best_split['feature index']\n",
    "        node.threshold = best_split['threshold']\n",
    "        left = best_split['left mask']\n",
    "        right = best_split['right mask']\n",
    "        node.truebranch = build_tree(X[left, :], y[left])\n",
    "        node.falsebranch = build_tree(X[right, :], y[right])\n",
    "    #不可再分, 叶子节点\n",
    "    else:\n",
    "        node.label = np.argmax(np.bincount(y))\n",
    "    \n",
    "    return node\n",
    "\n",
    "def predict(tree, X):\n",
    "    return [tree_predict(tree, row) for row in X]\n",
    "\n",
    "def tree_predict(tree, x):\n",
    "    # reached leaf\n",
    "    if tree.label is not None:\n",
    "        return tree.label\n",
    "    \n",
    "    prediction = None\n",
    "    # node has branches\n",
    "    # left? right?\n",
    "    if x[tree.feature_index] <= tree.threshold:\n",
    "        prediction = tree_predict(tree.truebranch, x)\n",
    "    else:\n",
    "        prediction = tree_predict(tree.falsebranch, x)\n",
    "    return prediction\n",
    "    \n",
    "\n",
    "decisionTree = build_tree(X, y)\n",
    "# print(decisionTree.truebranch)\n",
    "y_pred = predict(decisionTree, X)\n",
    "accuracy = (np.array(y_pred) ==  np.array(y) ).sum() / len(y)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
